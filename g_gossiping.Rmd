---
title: "g_gossiping"
output: html_notebook
---

#Deal with cookie
```{r}
url <- "https://www.ptt.cc/bbs/Gossiping/index.html"
res <- GET(url,config=set_cookies("over18"="1"))
res.string <- content(res,"text",encoding="utf8")

doc <- read_html(res.string)

css <- ".r-ent > .title >a"
node.a <- html_nodes(doc,css)

links <- html_attr(node.a,"href")

pre<- "https://www.ptt.cc"
links <- paste0(pre,links)
links
```

#to get the num of last page and all the links
```{r}
url <- "https://www.ptt.cc/bbs/Gossiping/index.html"
res <- GET(url,config=set_cookies("over18"="1"))
res.string <- content(res,"text",encoding="utf8")
doc <- read_html(res.string)

#Press on the "check" last page and copy the css selector
css<-"#action-bar-container > div > div.btn-group.btn-group-paging > a:nth-child(2)"

node.a <- html_node(doc,css)
last2.link <- html_attr(node.a,"href")

last2.num <- gsub(".*index([0-9]+).html","\\1.",last2.link)
endpage <- as.numeric(last2.num) + 1

all.link<-c()

for(p in 1:endpage){
  url <- sprintf("https://www.ptt.cc/bbs/Gossiping/index%s.html",p)
  res <- GET(url,config=set_cookies("over18"="1"))
  res.string <- content(res,"text",encoding="utf8")
  doc <- read_html(res.string)
  
  css<-".r-ent > .title >a"
  node.a<- html_nodes(doc,css)
  link<- html_attr(node.a,"href")
  pre<-"https://www.ptt.cc"
  #inks<-paste0(pre,link)
  
  all.link<-c(all.link,links)
  print(p)
}

```

#get all the titles during May ~ Jan
```{r}
library(dplyr)

all2.df<-data.frame()
#url<-post_url[1]

for(url in post_url[526836:663827]){
  tryCatch({
    res <- GET(url,config=set_cookies("over18"="1"))
    res.string <- content(res,"text",encoding="utf8")
    doc <- read_html(res.string)
    css<-"#main-content > div.article-metaline > span.article-meta-value"
    node.meta<-html_nodes(doc,css)
    meta<-html_text(node.meta)
    
    
    temp2.df<-data.frame(author=meta[1],
                         title=meta[2],
                         timestamp=meta[3],
                         url,
                         board="Gossiping")
    
    }, error = function(err) { 
    print("Error")
    print(url)
    })
    print(url)
    all2.df<-bind_rows(all2.df,temp2.df)  
}


which(post_url == "https://www.ptt.cc/bbs/Gossiping/M.1511534772.A.B00.html")
head(post_url)
saveRDS(all2.df, sprintf("data/titles_PTTGossiping_%s.rds", format(Sys.time(), "%Y%m%d-%H%M%S")))


#read data
all.titles<- readRDS("data/titles_PTTGossiping_20180127-165701.rds")
```

#清資料
```{r}
library(dplyr)
#remove duplicate rows
all.titles <- all.titles %>%
  distinct(all.titles)
#delete "m"
all.titles2 <- all.titles %>%
  slice(13061:664620)
```

#找"甲甲"文章
```{r}
gtiltes<- all.titles2[grep("甲甲", all.titles2$title), ]
saveRDS(gtiltes, sprintf("data/gtitles_PTTGossiping_%s.rds", format(Sys.time(), "%Y%m%d-%H%M%S")))
#dataframe -> character
glinks<-as.character(g.title$url)

#get all posts and comments

g.alldf <- data.frame()

for(url in glinks){
  tryCatch({
    res<-GET(url, config=set_cookies("over18"="1"))
    res.string<-content(res,"text", encoding="UTF-8")
    doc<-read_html(res.string, encoding="UTF-8")
    css1<-"#main-content"
    node.content<-html_nodes(doc,css1)
    maintext<-html_text(node.content)
    css2<-"#main-content > div.article-metaline> span.article-meta-value"
    node.meta<-html_nodes(doc,css2)
    meta<-html_text(node.meta)
    css3<-"#main-content > div.push > span.f3.hl.push-userid"
    node.pushid<-html_nodes(doc,css3)
    pushid<-html_text(node.pushid)
    css4<-"#main-content > div.push > span.f3.push-content"
    node.push<-html_nodes(doc,css4)
    pushcontent<-html_text(node.push)

    temp.df<-data.frame(cauthor=pushid,
                        comment=pushcontent,
                        maintext,
                        author=meta[1],
                        title=meta[2],
                        timestamp=meta[3],
                        url,
                        board="Gossiping")
    
    
  }, error = function(err) { 
    #print("Error")
    #print(url)
  })
  print(url)
  g.alldf<-bind_rows(g.alldf,temp.df)  
}
```

#從maintext清出post
```{r}
#get the posts
post.alldf <- data.frame()

for (x in g.alldf$maintext[1:91339]){
  gsub("\n","",x)
  StartName<-regexpr("2017",x)
  EndName<- regexpr("※",x)
  post1<-substr(x, StartName+4, EndName-1)
  post1<-gsub("--","",post1)
  temp3.df<-data.frame(post=post1)
  post.alldf<-bind_rows(post.alldf,temp3.df) 
}

for (i in g.alldf$maintext[91340:92401]){
  gsub("\n","",i)
  StartName<-regexpr("2018",i)
  EndName<- regexpr("※",i)
  post1<-substr(i, StartName+4, EndName-1)
  post1<-gsub("--","",post1)
  temp3.df<-data.frame(post=post1)
  post.alldf<-bind_rows(post.alldf,temp3.df) 
}

g.alldf<-cbind(g.alldf, post.alldf)
```

#處理missing data
```{r}
#find out missing
TEST <- data.frame(TF = g.alldf$post[1:92401] == "\n")
TEST$num <- rep( 1:nrow(g.alldf))
g.alldf.TEST <- cbind(TEST,g.alldf)
g.alldf.TEST <- g.alldf.TEST %>%
  filter(TF == "TRUE")

TESTdf<- data.frame()
#fill missing
for (i in g.alldf.TEST$maintext){
  gsub("\n","",i)
  StartName<-regexpr("※",i)
  EndName<- regexpr("--",i)
  post1<-substr(i, StartName, EndName-2)
  temp3.df<-data.frame(fullpost=post1)
  TESTdf<-bind_rows(TESTdf,temp3.df) 
}
g.alldf.TEST <- cbind(g.alldf.TEST,TESTdf)
g.alldf$num <- rep( 1:nrow(g.alldf))
twocol <- data.frame(g.alldf.TEST$num,g.alldf.TEST$fullpost)
names(twocol)[names(twocol)=="g.alldf.TEST.num"]="num" #key

#join by key
g.alldf<-dplyr::full_join(g.alldf,twocol,by="num")
indx <- g.alldf$post == "\n"
g.alldf$post[indx] <- g.alldf$g.alldf.TEST.fullpost[indx]
g.alldf <- g.alldf[,-11]

saveRDS(g.alldf, sprintf("data/g.alldf_PTTGossiping_%s.rds", format(Sys.time(), "%Y%m%d-%H%M%S")))
```

#Importing package
```{r}

library(jiebaR)
library(topicmodels)
library(tidyr)
library(stringr)
library(tidytext)

```

# Count the frequency
```{r}
library(stringr)
word_count <- word_token %>%
	filter(!is.na(words), words != "") %>%
	count(words) %>% #超重要每個字在每一年出現幾次
	ungroup() %>%
  filter(n>5) %>%
	filter(!str_detect(words, "[a-zA-Z0-9]+")) %>% #英文字濾掉
	filter(!(words %in% stopwords)) #濾stopWords
```

#Visualize
```{r}
word_count %>%
	top_n(15) %>%
	ggplot(aes(x = reorder(words,n),y = n)) + 
	geom_bar(stat = "identity",fill = "#00CED1")+
  xlab('words')+
  coord_flip()
```


#post LDA
```{r}
#Word segmentation
cutter <- worker()
segment_not <- c("甲甲","霸凌","肛肛")
new_user_word(cutter, segment_not)

#Get all posts
POST<-data.frame(g.alldf$post)
#remove duplicate rows
POST <- POST %>%
  distinct(g.alldf.post)
POST$id <- rep( 1:nrow(POST))

POST$words <- sapply(POST$g.alldf.post, function(x){tryCatch({cutter[x]}, error=function(err){})})

#Loading stopWords
fin <- file("stopwords_tw.txt",open="r")
stopwords<-readLines(fin,encoding="UTF8")
stopwords<-unique(stopwords)

#Tokenizing
library(tidyr) # for unnest()
library(stringr)
word_token <- POST %>%
  unnest() %>%
  select(id,words) %>%
  filter(!(words %in% stopwords))%>%
  filter(!str_detect(words,"\\d")) %>%
  filter(nchar(words)>1)%>%
  filter(words!="就是")%>%
  filter(words!="怎麼")%>%
  filter(words!="什麼")%>%
  filter(words!="不會")%>%
  filter(words!="真的")%>%
  filter(words!="一個")%>%
  filter(words!="沒有")%>%
  filter(words!="不是")%>%
  filter(words!="所以")%>%
  filter(words!="知道")%>%
  filter(words!="這樣")%>%
  filter(words!="可以")%>%
  filter(words!="現在")%>%
  filter(words!="自己")%>%
  filter(words!="這樣")%>%
  filter(words!="還是")%>%
  filter(words!="因為")%>%
  filter(words!="com")%>%
  filter(words!="imgur")%>%
  filter(words!="www")%>%
  filter(words!="https")%>%
  filter(words!="jpg")%>%
  filter(words!="http")%>%
  filter(words!="Gossiping")%>%
  filter(words!="html")%>%
  filter(words!="ptt")%>%
  filter(words!="from")%>%
  filter(words!="my")%>%
  filter(words!="sent")%>%
  filter(words!="on")%>%
  filter(words!="覺得")%>%
  filter(words!="結果")%>%
  filter(words!="然後")%>%
  filter(words!="結果")%>%
  filter(words!="時候")%>%
  filter(words!="引述")%>%
  filter(words!="如題")%>%
  filter(words!="bbs")%>%
  filter(words!="JPTT")%>%
  filter(words!="cc")%>%
  filter(words!="看到")%>%
  filter(words!="八卦")%>%
  filter(words!="這個")%>%
  filter(words!="Vdragon")%>%
  filter(words!="goo")%>%
  filter(words!="gl")%>%
  filter(words!="一直")%>%
  filter(words!="開始")%>%
  filter(words!="問卦")%>%
  filter(words!="有沒有")%>%
  filter(words!="應該")%>%
  filter(words!="這種")%>%
  filter(words!="Sent")%>%
  filter(words!="有人")%>%
  filter(words!="不要")%>%
  filter(words!="der")%>%
  filter(words!="一樣")%>%
  filter(words!="今天")%>%
  filter(words!="可能")%>%
  filter(words!="那麼")%>%
  filter(words!="最近")%>%
  filter(words!="是不是")%>%
  filter(words!="以下")%>%
  filter(words!="一下")%>%
  filter(words!="只是")%>%
  filter(words!="但是")%>%
  filter(words!="請問")%>%
  filter(words!="male")%>%
  filter(words!="大家")%>%
  filter(words!="如果")%>%
  filter(words!="一堆")


library(tidytext)
dtm_g <- word_token %>%
	count(id, words) %>%
  cast_dtm(id, words, n)

library(topicmodels)
dtm_g14 <- LDA(dtm_g, k = 14, control = list(seed = 1234))

library(ggplot2)
dtm_topics_g <- tidy(dtm_g14, matrix = "beta")
top_terms_g <- dtm_topics_g %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)
View(top_terms_g)
top_terms_g %>%
    mutate(term = reorder(term, beta)) %>%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip() +
    theme(axis.text.y=element_text(colour="black", family="Heiti TC Light"))

```

#frequency of comments
```{r}
#Word segmentation
cutter <- worker()
segment_not <- c("甲甲","霸凌","肛肛","不正常","性行為")
new_user_word(cutter, segment_not)

#Get all posts
comments<-data.frame(g.alldf$comment)

comments$g.alldf.comment <- sapply(comments$g.alldf.comment, function(x){tryCatch({cutter[x]}, error=function(err){})})
names(comments)<-c("words")

#Loading stopWords
fin <- file("stopwords_tw.txt",open="r")
stopwords<-readLines(fin,encoding="UTF8")
stopwords<-unique(stopwords)

#Tokenizing
library(tidyr) # for unnest()
library(stringr)
word_token_c <- comments %>%
  unnest() %>%
  select(words) %>%
  filter(!(words %in% stopwords))%>%
  filter(!str_detect(words,"\\d")) %>%
  filter(nchar(words)>1)%>%
  filter(words!="就是")%>%
  filter(words!="怎麼")%>%
  filter(words!="什麼")%>%
  filter(words!="不會")%>%
  filter(words!="真的")%>%
  filter(words!="一個")%>%
  filter(words!="沒有")%>%
  filter(words!="不是")%>%
  filter(words!="所以")%>%
  filter(words!="知道")%>%
  filter(words!="這樣")%>%
  filter(words!="可以")%>%
  filter(words!="現在")%>%
  filter(words!="自己")%>%
  filter(words!="這樣")%>%
  filter(words!="還是")%>%
  filter(words!="因為")%>%
  filter(words!="com")%>%
  filter(words!="imgur")%>%
  filter(words!="www")%>%
  filter(words!="https")%>%
  filter(words!="jpg")%>%
  filter(words!="http")%>%
  filter(words!="Gossiping")%>%
  filter(words!="html")%>%
  filter(words!="ptt")%>%
  filter(words!="from")%>%
  filter(words!="my")%>%
  filter(words!="sent")%>%
  filter(words!="on")%>%
  filter(words!="覺得")%>%
  filter(words!="結果")%>%
  filter(words!="然後")%>%
  filter(words!="結果")%>%
  filter(words!="時候")%>%
  filter(words!="引述")%>%
  filter(words!="如題")%>%
  filter(words!="bbs")%>%
  filter(words!="JPTT")%>%
  filter(words!="cc")%>%
  filter(words!="看到")%>%
  filter(words!="八卦")%>%
  filter(words!="這個")%>%
  filter(words!="Vdragon")%>%
  filter(words!="goo")%>%
  filter(words!="gl")%>%
  filter(words!="一直")%>%
  filter(words!="開始")%>%
  filter(words!="問卦")%>%
  filter(words!="有沒有")%>%
  filter(words!="應該")%>%
  filter(words!="這種")%>%
  filter(words!="Sent")%>%
  filter(words!="樓上")%>%
  filter(words!="五樓")%>%
  filter(words!="有人")%>%
  filter(words!="不要")%>%
  filter(words!="der")%>%
  filter(words!="一樣")%>%
  filter(words!="今天")%>%
  filter(words!="出來")%>%
  filter(words!="可能")%>%
  filter(words!="那麼")%>%
  filter(words!="這麼")%>%
  filter(words!="最近")%>%
  filter(words!="是不是")%>%
  filter(words!="以下")%>%
  filter(words!="一下")%>%
  filter(words!="只是")%>%
  filter(words!="但是")%>%
  filter(words!="請問")%>%
  filter(words!="male")%>%
  filter(words!="大家")%>%
  filter(words!="如果")%>%
  filter(words!="根本")%>%
  filter(words!="比較")%>%
  filter(words!="不能")%>%
  filter(words!="一定")%>%
  filter(words!="一堆")

```

#Count the frequency and visualize
```{r}
word_count_c <- word_token_c %>%
	filter(!is.na(words), words != "") %>%
	count(words) %>% #超重要每個字在每一年出現幾次
	ungroup() %>%
  filter(n>5) %>%
	filter(!str_detect(words, "[a-zA-Z0-9]+")) %>% #英文字濾掉
	filter(!(words %in% stopwords)) #濾stopWords

word_count_c %>%
	top_n(30) %>%
	ggplot(aes(x = reorder(words,n),y = n)) + 
	geom_bar(stat = "identity",fill = "#00CED1")+
  xlab('words')+
  coord_flip()
```

